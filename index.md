---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: main
title: "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code"
---

![Image](static/images/output_cropped.svg){:width="100%" style="max-width: 8000px;"}
{:style="text-align: center;"}
<!-- *Figure 1: Overview of the  evaluation process* -->
**ResearchCodeBench** *collects recently released research* **papers** *and their corresponding* **code** *repository to construct a benchmark for evaluating code LLMs on research code generation.*
{:style="text-align: center;"}

# Introduction

ResearchCodeBench is a rigorous benchmark for evaluating LLMs on their ability to translate novel machine learning research papers into executable code. It is built from 212 real-world coding challenges sourced directly from top ML papers published in 2024â€“2025 and continues to grow through ongoing community contributions. We evaluate over 30 proprietary and open-source models and find that even the best-performing LLMs solve fewer than 40% of the challenges. In addition to performance scores, ResearchCodeBench presents novel empirical findings not captured by prior benchmarks.


# Contamination






# Help Us Grow ResearchCodeBench!

We're looking for recent ML papers with available code to add to our benchmark. Paper2Code-bench is continuously evolving, and we welcome contributions from the research community to expand our collection of papers and implementation challenges.

Help us identify the core contributions within the code that test LLMs' ability to implement novel ideas. Your expertise will help shape the future of AI evaluation benchmarks.

### What We Need:
- **Paper Link** (arXiv, conference page, etc.)
- **Official GitHub Repository Link**
- **Your thoughts on the key code sections/lines to focus on**

### How to Submit:
You can submit your contribution through our [online form](https://forms.gle/xZarsnAa6a2u43Tr6). Or scan the QR code blow:
![qr_code](static/images/qr_code.png){:width="200px" style="display: block; margin: 0 auto;"}

### Contributors Get Credited!
All contributors will be acknowledged in our publications and on the official Paper2Code-bench website.

#### Why Your Contribution Matters
By contributing to Paper2Code-bench, you're helping advance the development of AI systems that can understand and implement novel research ideas. This benchmark serves as a critical evaluation tool for measuring progress toward more capable AI research assistants.

Join us in building the future of AI research tools! 



## List of papers

1. **Advantage Alignment Algorithms**  
   [Paper](https://arxiv.org/abs/2406.14662v3) | [Code](https://github.com/jduquevan/advantage-alignment)

2. **Differential Transformer**  
   [Paper](https://arxiv.org/abs/2410.05258v2) | [Code](https://aka.ms/Diff-Transformer)

3. **Diffusion Model Alignment Using Direct Preference Optimization**  
   [Paper](https://arxiv.org/abs/2311.12908v1) | [Code](https://github.com/SalesforceAIResearch/DiffusionDPO)

4. **Transformers without Normalization**  
   [Paper](https://www.arxiv.org/abs/2503.10622v1) | [Code](https://github.com/jiachenzhu/DyT)

5. **Your ViT is Secretly an Image Segmentation Model**  
   [Paper](https://arxiv.org/abs/2503.19108v1) | [Code](https://github.com/tue-mps/eomt/)

6. **Fractal Generative Models**  
   [Paper](https://arxiv.org/abs/2502.17437v2) | [Code](https://github.com/LTH14/fractalgen)

7. **Gaussian Mixture Flow Matching Models**  
   [Paper](https://arxiv.org/abs/2504.05304v1) | [Code](https://github.com/Lakonik/GMFlow)

8. **GPS: A Probabilistic Distributional Similarity with Gumbel Priors for Set-to-Set Matching**  
   [Paper](https://openreview.net/forum?id=U0SijGsCHJ) | [Code](https://github.com/Zhang-VISLab/ICLR2025-GPS)

9. **On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding**  
   [Paper](https://arxiv.org/abs/2405.16865v4) | [Code](https://github.com/DehongXu/grid-cell-conformal-isometry)

10. **Attention as a Hypernetwork**  
    [Paper](https://arxiv.org/abs/2406.05816v4) | [Code](https://github.com/smonsays/hypernetwork-attention)

11. **Second-Order Min-Max Optimization with Lazy Hessians**  
    [Paper](https://arxiv.org/abs/2410.09568v1) | [Code](https://github.com/TrueNobility303/LEN)

12. **Mapping the increasing use of LLMs in scientific papers**  
    [Paper](https://arxiv.org/abs/2404.01268v1) | [Code](https://github.com/Weixin-Liang/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers)

13. **Min-p Sampling for Creative and Coherent LLM Outputs**  
    [Paper](https://arxiv.org/abs/2407.01082v4) | [Code](https://github.com/menhguin/minp_paper)

14. **Optimal Stepsize for Diffusion Sampling**  
    [Paper](https://arxiv.org/abs/2503.21774v1) | [Code](https://github.com/bebebe666/OptimalSteps)

15. **REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers**  
    [Paper](https://arxiv.org/abs/2504.10483v1) | [Code](https://github.com/jduquevan/REPA-E)

16. **The Road Less Scheduled**  
    [Paper](https://arxiv.org/abs/2405.15682v4) | [Code](https://github.com/facebookresearch/schedule_free)

17. **Principal Components Enable A New Language of Images**  
    [Paper](https://arxiv.org/abs/2503.08685v1) | [Code](https://github.com/visual-gen/semanticist)

18. **Data Unlearning in Diffusion Models**  
    [Paper](https://arxiv.org/abs/2503.01034) | [Code](https://github.com/claserken/SISS)

19. **TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation**  
    [Paper](https://arxiv.org/abs/2410.20626v3) | [Code](https://github.com/MinkaiXu/TabDiff)

20. **Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis**  
    [Paper](https://arxiv.org/abs/2410.02242v2) | [Code](https://github.com/1HyunwooLee/Tanh-Init)
